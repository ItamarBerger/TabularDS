{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fX_NCznlNHye"
      },
      "source": [
        "# preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ItamarBerger/TabularDS"
      ],
      "metadata": {
        "id": "pi_KucCvRDGx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load dataset\n",
        "import pandas as pd\n",
        "data = pd.read_csv('/content/TabularDS/final_project/code/data/online_retail.csv')\n",
        "# data = retails.copy()"
      ],
      "metadata": {
        "id": "dZwX40EVPMdy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# retails.head()\n",
        "# print(len(retails))\n",
        "data.head()"
      ],
      "metadata": {
        "id": "zVGaG2KbQL92"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Apriori for rule mining"
      ],
      "metadata": {
        "id": "rU_fNB6EUTNP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data['InvoiceDate'] = pd.to_datetime(data['InvoiceDate'])\n",
        "data.dropna(subset=['InvoiceNo', 'Description'], inplace=True)\n",
        "\n",
        "transactions = data.groupby('InvoiceNo')['Description'].apply(list).tolist()"
      ],
      "metadata": {
        "id": "2tLlnwOvabf-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from mlxtend.frequent_patterns import apriori, association_rules\n",
        "from mlxtend.preprocessing import TransactionEncoder\n",
        "\n",
        "# Sample a smaller subset (increase size if needed)\n",
        "transactions = transactions[:200]\n",
        "\n",
        "# Optimize TransactionEncoder using sparse format\n",
        "te = TransactionEncoder()\n",
        "te_ary = te.fit(transactions).transform(transactions, sparse=True)\n",
        "df = pd.DataFrame.sparse.from_spmatrix(te_ary, columns=te.columns_)\n",
        "\n",
        "# Apply Apriori with a higher support threshold (adjust as needed)\n",
        "frequent_itemsets = apriori(df, min_support=0.05, use_colnames=True)  # Try 0.05 or higher\n",
        "\n",
        "# Generate association rules\n",
        "if not frequent_itemsets.empty:\n",
        "    rules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=1.0)\n",
        "    print(rules)\n",
        "else:\n",
        "    print(\"No frequent itemsets found. Try lowering min_support.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "Zrk63WVodLtC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# top rules by lift\n",
        "# top_lift_rules = rules.sort_values('lift', ascending=False).head(100)\n",
        "# print(top_lift_rules)\n"
      ],
      "metadata": {
        "id": "Mch0QcYun0r-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "# import networkx as nx\n",
        "# from mlxtend.frequent_patterns import apriori, association_rules\n",
        "\n",
        "# # Create a directed graph\n",
        "# G = nx.DiGraph()\n",
        "# narrow_rules = rules[0:25]\n",
        "# # Add nodes and directed edges\n",
        "# for _, rule in narrow_rules.iterrows():\n",
        "#     antecedent = ', '.join(list(rule['antecedents']))\n",
        "#     consequent = ', '.join(list(rule['consequents']))\n",
        "\n",
        "#     # Add a directed edge from antecedent to consequent\n",
        "#     G.add_edge(antecedent, consequent, weight=rule['lift'], support=rule['support'], confidence=rule['confidence'])\n",
        "\n",
        "# # Draw the directed graph\n",
        "# plt.figure(figsize=(12, 12))\n",
        "# pos = nx.spring_layout(G, k=0.3, iterations=20)  # Layout for the graph\n",
        "\n",
        "# # Draw the nodes and edges\n",
        "# nx.draw_networkx_nodes(G, pos, node_size=200, node_color='lightblue', alpha=0.7)\n",
        "# nx.draw_networkx_edges(G, pos, width=2.0, alpha=0.5, arrowstyle='->', arrowsize=10)\n",
        "# nx.draw_networkx_labels(G, pos, font_size=10, font_color='black')\n",
        "\n",
        "# # Add edge labels (optional, if you want to display lift, confidence, or support)\n",
        "# # edge_labels = nx.get_edge_attributes(G, 'weight')\n",
        "# # nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels)\n",
        "\n",
        "# plt.title(\"Association Rules Directed Graph\")\n",
        "# plt.axis('off')\n",
        "# plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "4pHXBq27f-Lb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import plotly.graph_objects as go\n",
        "# import networkx as nx\n",
        "# from mlxtend.frequent_patterns import apriori, association_rules\n",
        "\n",
        "# # Example association rules (you need to load your own rules here)\n",
        "# # Example: rules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=1.0)\n",
        "\n",
        "# # Create a directed graph\n",
        "# G = nx.DiGraph()\n",
        "\n",
        "# # Add nodes and directed edges\n",
        "# for _, rule in narrow_rules.iterrows():\n",
        "#     antecedent = ', '.join(list(rule['antecedents']))\n",
        "#     consequent = ', '.join(list(rule['consequents']))\n",
        "\n",
        "#     # Add a directed edge from antecedent to consequent\n",
        "#     G.add_edge(antecedent, consequent, weight=rule['lift'], support=rule['support'], confidence=rule['confidence'])\n",
        "\n",
        "# # Get positions of nodes using a layout\n",
        "# pos = nx.spring_layout(G, k=0.3, iterations=20)\n",
        "\n",
        "# # Get edge and node information for Plotly\n",
        "# edge_x = []\n",
        "# edge_y = []\n",
        "# for edge in G.edges():\n",
        "#     x0, y0 = pos[edge[0]]\n",
        "#     x1, y1 = pos[edge[1]]\n",
        "#     edge_x.append(x0)\n",
        "#     edge_y.append(y0)\n",
        "#     edge_x.append(x1)\n",
        "#     edge_y.append(y1)\n",
        "\n",
        "# # Node positions\n",
        "# node_x = [pos[node][0] for node in G.nodes()]\n",
        "# node_y = [pos[node][1] for node in G.nodes()]\n",
        "\n",
        "# # Create hover text for nodes (empty text for default)\n",
        "# hover_text = [node for node in G.nodes()]\n",
        "\n",
        "# # Create the plotly figure\n",
        "# fig = go.Figure()\n",
        "\n",
        "# # Add directed edges with arrows (using annotations for arrows)\n",
        "# for edge in G.edges():\n",
        "#     x0, y0 = pos[edge[0]]\n",
        "#     x1, y1 = pos[edge[1]]\n",
        "\n",
        "#     # Add edge line\n",
        "#     fig.add_trace(go.Scatter(x=[x0, x1], y=[y0, y1],\n",
        "#                              line=dict(width=1, color='gray'),\n",
        "#                              mode='lines',\n",
        "#                              showlegend=False))\n",
        "\n",
        "#     # Add arrow (using annotations)\n",
        "#     fig.add_annotation(\n",
        "#         x=x1, y=y1,\n",
        "#         ax=x0, ay=y0,\n",
        "#         axref=\"x\", ayref=\"y\", xref=\"x\", yref=\"y\",\n",
        "#         showarrow=True, arrowhead=2, arrowsize=1,\n",
        "#         arrowcolor='gray', opacity=0.7\n",
        "#     )\n",
        "\n",
        "# # Add nodes to the plot\n",
        "# fig.add_trace(go.Scatter(x=node_x, y=node_y,\n",
        "#                          mode='markers',\n",
        "#                          hoverinfo='text',  # This ensures text shows on hover only\n",
        "#                          hovertext=hover_text,  # Set hovertext to show item names on hover\n",
        "#                          marker=dict(color='skyblue', size=10, line=dict(width=1, color='black'))))\n",
        "\n",
        "# # Add edge labels (optional: you can use lift or other metrics)\n",
        "# edge_labels = nx.get_edge_attributes(G, 'weight')\n",
        "# for edge, label in edge_labels.items():\n",
        "#     x0, y0 = pos[edge[0]]\n",
        "#     x1, y1 = pos[edge[1]]\n",
        "#     fig.add_trace(go.Scatter(x=[(x0+x1)/2], y=[(y0+y1)/2],\n",
        "#                              text=[f'Lift: {label:.2f}'],\n",
        "#                              mode='text',\n",
        "#                              showlegend=False))\n",
        "\n",
        "# # Update layout settings for better visualization\n",
        "# fig.update_layout(title=\"Association Rules Directed Graph (Interactive)\",\n",
        "#                   showlegend=False,\n",
        "#                   hovermode='closest',\n",
        "#                   xaxis=dict(showgrid=False, zeroline=False),\n",
        "#                   yaxis=dict(showgrid=False, zeroline=False))\n",
        "\n",
        "# # Show the plot\n",
        "# fig.show()\n"
      ],
      "metadata": {
        "id": "IJrqXXbgiEl0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import seaborn as sns\n",
        "\n",
        "# # Plot support vs confidence\n",
        "# plt.figure(figsize=(10, 6))\n",
        "# sns.scatterplot(x='support', y='confidence', data=rules, hue='lift', palette='coolwarm', size='lift', sizes=(50, 300))\n",
        "# plt.title(\"Support vs Confidence with Lift\")\n",
        "# plt.xlabel('Support')\n",
        "# plt.ylabel('Confidence')\n",
        "# plt.show()\n",
        "\n",
        "# # Plot lift distribution\n",
        "# plt.figure(figsize=(10, 6))\n",
        "# sns.histplot(rules['lift'], kde=True)\n",
        "# plt.title(\"Lift Distribution\")\n",
        "# plt.xlabel('Lift')\n",
        "# plt.ylabel('Frequency')\n",
        "# plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "id": "p1uXodl4gAMw",
        "outputId": "3d18758f-1914-407f-b91e-b88497315619"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-f6ec390fedc8>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Plot support vs confidence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatterplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'support'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'confidence'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrules\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'lift'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpalette\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'coolwarm'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'lift'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msizes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Support vs Confidence with Lift\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Support'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/seaborn/relational.py\u001b[0m in \u001b[0;36mscatterplot\u001b[0;34m(data, x, y, hue, size, style, palette, hue_order, hue_norm, sizes, size_order, size_norm, markers, style_order, legend, ax, **kwargs)\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"color\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_default_color\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 636\u001b[0;31m     \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    637\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/seaborn/relational.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, ax, kws)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"hue\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m             \u001b[0mpoints\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_facecolors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hue_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"hue\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"size\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/matplotlib/_api/__init__.py\u001b[0m in \u001b[0;36mmethod\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/matplotlib/collections.py\u001b[0m in \u001b[0;36mset_facecolor\u001b[0;34m(self, c)\u001b[0m\n\u001b[1;32m    794\u001b[0m             \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    795\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_facecolor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 796\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_facecolor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    797\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    798\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_facecolor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/matplotlib/collections.py\u001b[0m in \u001b[0;36m_set_facecolor\u001b[0;34m(self, c)\u001b[0m\n\u001b[1;32m    776\u001b[0m             \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_default_facecolor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 778\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_facecolors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmcolors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_rgba_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_alpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    779\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/matplotlib/colors.py\u001b[0m in \u001b[0;36mto_rgba_array\u001b[0;34m(c, alpha)\u001b[0m\n\u001b[1;32m    510\u001b[0m             \u001b[0mrgba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 512\u001b[0;31m             \u001b[0mrgba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mto_rgba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    513\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m         \u001b[0mrgba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mto_rgba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/matplotlib/colors.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    510\u001b[0m             \u001b[0mrgba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 512\u001b[0;31m             \u001b[0mrgba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mto_rgba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    513\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m         \u001b[0mrgba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mto_rgba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/matplotlib/colors.py\u001b[0m in \u001b[0;36mto_rgba\u001b[0;34m(c, alpha)\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[0mrgba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrgba\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Suppress exception chaining of cache lookup failure.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m         \u001b[0mrgba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_to_rgba_no_colorcycle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m             \u001b[0m_colors_full_map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrgba\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/matplotlib/colors.py\u001b[0m in \u001b[0;36m_to_rgba_no_colorcycle\u001b[0;34m(c, alpha)\u001b[0m\n\u001b[1;32m    401\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RGBA sequence should have length 3 or 4\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mReal\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    404\u001b[0m         \u001b[0;31m# Checks that don't work: `map(float, ...)`, `np.array(..., float)` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m         \u001b[0;31m# `np.array(...).astype(float)` would all convert \"0.5\" to 0.5.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/matplotlib/colors.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    401\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RGBA sequence should have length 3 or 4\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mReal\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    404\u001b[0m         \u001b[0;31m# Checks that don't work: `map(float, ...)`, `np.array(..., float)` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m         \u001b[0;31m# `np.array(...).astype(float)` would all convert \"0.5\" to 0.5.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/abc.py\u001b[0m in \u001b[0;36m__instancecheck__\u001b[0;34m(cls, instance)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# calc stats for baseline apriori\n",
        "mean_supp = rules['support'].mean()\n",
        "mean_conf = rules['confidence'].mean()\n",
        "mean_lift = rules['lift'].mean()\n",
        "print(f'''Mean Support: {mean_supp}\n",
        "Mean Confidence: {mean_conf}\n",
        "Mean Lift: {mean_lift}''')\n"
      ],
      "metadata": {
        "id": "6rCHjQyimCfZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cluster items + Apriori\n"
      ],
      "metadata": {
        "id": "-qqhjLNWXnsL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from mlxtend.frequent_patterns import apriori, association_rules\n",
        "from sklearn.cluster import KMeans\n",
        "from scipy.spatial.distance import cdist\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from mlxtend.preprocessing import TransactionEncoder\n",
        "from sklearn.cluster import KMeans\n",
        "from scipy.spatial.distance import cdist\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ðŸ”¹ Step 1: Convert Transactions to One-Hot Encoded Format\n",
        "te = TransactionEncoder()\n",
        "one_hot = te.fit_transform(transactions)\n",
        "df = pd.DataFrame(one_hot, columns=te.columns_)\n",
        "\n",
        "# ðŸ”¹ Step 2: Determine Optimal Number of Clusters (Elbow Method)\n",
        "distortions = []\n",
        "K_range = range(2, 10)  # Try different values of K\n",
        "\n",
        "for k in K_range:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    kmeans.fit(df)\n",
        "    distortions.append(sum(np.min(cdist(df, kmeans.cluster_centers_, 'euclidean'), axis=1)) / df.shape[0])\n",
        "\n",
        "# ðŸ”¹ Step 3: Plot the Elbow Curve\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(K_range, distortions, marker='o', linestyle='--')\n",
        "plt.xlabel('Number of Clusters (k)')\n",
        "plt.ylabel('Distortion')\n",
        "plt.title('Elbow Method for Optimal k')\n",
        "plt.show()\n",
        "\n",
        "# ðŸ”¹ Step 4: Fit K-Means with Optimal k\n",
        "optimal_k = 4  # Choose based on the Elbow Method\n",
        "kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
        "df['Cluster'] = kmeans.fit_predict(df)\n",
        "\n",
        "# ðŸ”¹ Step 5: Print Cluster Assignments\n",
        "print(df[['Cluster']])\n"
      ],
      "metadata": {
        "id": "zJds2I7wXq9T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_results = {}\n",
        "\n",
        "for cluster_id in sorted(df['Cluster'].unique()):\n",
        "    print(f\"\\nProcessing Cluster {cluster_id}...\")\n",
        "\n",
        "    # Extract transactions for the cluster\n",
        "    cluster_data = df[df['Cluster'] == cluster_id].drop(columns=['Cluster'])\n",
        "\n",
        "    # Run Apriori\n",
        "    frequent_itemsets = apriori(cluster_data, min_support=0.05, use_colnames=True)\n",
        "    rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.05)\n",
        "\n",
        "    # Store results\n",
        "    cluster_results[cluster_id] = {\n",
        "        \"frequent_itemsets\": frequent_itemsets,\n",
        "        \"rules\": rules\n",
        "    }\n",
        "\n",
        "    print(f\"Cluster {cluster_id}: {len(rules)} rules generated.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "dOb8TpaDX2Cg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Initialize dictionary to store average metrics per cluster\n",
        "cluster_metrics = {}\n",
        "\n",
        "for cluster_id, data in cluster_results.items():\n",
        "    rules = data[\"rules\"]\n",
        "\n",
        "    if not rules.empty:\n",
        "        avg_support = rules[\"support\"].mean()\n",
        "        avg_confidence = rules[\"confidence\"].mean()\n",
        "        avg_lift = rules[\"lift\"].mean()\n",
        "    else:\n",
        "        avg_support, avg_confidence, avg_lift = 0, 0, 0  # Handle empty clusters\n",
        "\n",
        "    cluster_metrics[cluster_id] = {\n",
        "        \"Avg Support\": avg_support,\n",
        "        \"Avg Confidence\": avg_confidence,\n",
        "        \"Avg Lift\": avg_lift\n",
        "    }\n",
        "\n",
        "# Convert dictionary to DataFrame for easier plotting\n",
        "metrics_df = pd.DataFrame.from_dict(cluster_metrics, orient=\"index\")\n",
        "print(metrics_df)\n"
      ],
      "metadata": {
        "id": "HEVQidHkX6ft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using item-based CF for rules mining"
      ],
      "metadata": {
        "id": "T2N5TnA2kZdm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "yS_Pq9QcdKTy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.spatial.distance import pdist, squareform\n",
        "\n",
        "transactions = transactions\n",
        "\n",
        "\n",
        "def calculate_jaccard_similarity(transaction_matrix):\n",
        "    \"\"\"Calculate Jaccard similarity between all item pairs\"\"\"\n",
        "    binary_matrix = (transaction_matrix > 0).astype(int)\n",
        "    jaccard_dist = pdist(binary_matrix.T, metric='jaccard')\n",
        "    jaccard_sim = 1 - squareform(jaccard_dist)\n",
        "    return pd.DataFrame(jaccard_sim, index=transaction_matrix.columns, columns=transaction_matrix.columns)\n",
        "\n",
        "def calculate_item_means(transaction_matrix):\n",
        "    \"\"\"Calculate mean occurrence (probability) for each item\"\"\"\n",
        "    return transaction_matrix.mean()\n",
        "\n",
        "def calculate_recommendation_score(transaction_items, candidate_item, item_similarity_df, transaction_matrix):\n",
        "    \"\"\"\n",
        "    Calculate recommendation score using adjusted formula for binary ratings:\n",
        "    score(u,i) = Î£j similarity(i,j)(r(u,j) - rÌ„j) / Î£j |similarity(i,j)| + rÌ„i\n",
        "    where:\n",
        "    - r(u,j) is 1 if item j is in transaction, 0 otherwise\n",
        "    - rÌ„j is the mean occurrence of item j across all transactions\n",
        "    - rÌ„i is the mean occurrence of candidate item i\n",
        "    \"\"\"\n",
        "    # Get mean occurrences for all items\n",
        "    item_means = calculate_item_means(transaction_matrix)\n",
        "\n",
        "    numerator = 0\n",
        "    denominator = 0\n",
        "\n",
        "    # Calculate the weighted sum of deviations from mean\n",
        "    for item in transaction_items:\n",
        "        similarity = item_similarity_df.loc[candidate_item, item]\n",
        "        # r(u,j) is 1 since item is in transaction\n",
        "        rating_deviation = 1 - item_means[item]  # (r(u,j) - rÌ„j)\n",
        "        numerator += similarity * rating_deviation\n",
        "        denominator += abs(similarity)\n",
        "\n",
        "    # Add the candidate item's mean occurrence\n",
        "    mean_i = item_means[candidate_item]\n",
        "\n",
        "    if denominator != 0:\n",
        "        score = (numerator / denominator) + mean_i\n",
        "    else:\n",
        "        score = mean_i\n",
        "\n",
        "    return score\n",
        "\n",
        "def generate_transaction_rules(transactions):\n",
        "    # Create binary transaction matrix\n",
        "    item_list = sorted(set([item for sublist in transactions for item in sublist]))\n",
        "    transaction_matrix = pd.DataFrame(0, index=range(len(transactions)), columns=item_list)\n",
        "    for idx, transaction in enumerate(transactions):\n",
        "        for item in transaction:\n",
        "            transaction_matrix.at[idx, item] = 1\n",
        "\n",
        "    # Calculate item similarity\n",
        "    item_similarity_df = calculate_jaccard_similarity(transaction_matrix)\n",
        "\n",
        "    # Generate recommendations\n",
        "    recommendations = []\n",
        "\n",
        "    for transaction in transactions:\n",
        "        scores = {}\n",
        "\n",
        "        # Calculate scores for all possible items not in transaction\n",
        "        for candidate_item in item_list:\n",
        "            if candidate_item not in transaction:\n",
        "                score = calculate_recommendation_score(\n",
        "                    transaction,\n",
        "                    candidate_item,\n",
        "                    item_similarity_df,\n",
        "                    transaction_matrix\n",
        "                )\n",
        "                scores[candidate_item] = score\n",
        "\n",
        "        if scores:\n",
        "            # Get best recommendation\n",
        "            best_item = max(scores.items(), key=lambda x: x[1])\n",
        "\n",
        "            recommendation = {\n",
        "                'transaction': transaction,\n",
        "                'recommended_item': best_item[0],\n",
        "                'score': best_item[1]\n",
        "            }\n",
        "            recommendations.append(recommendation)\n",
        "\n",
        "    return pd.DataFrame(recommendations)\n",
        "\n",
        "\n",
        "# Usage\n",
        "rules_df = generate_transaction_rules(transactions)\n",
        "\n"
      ],
      "metadata": {
        "id": "rK22Gt1sd1SM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rules_df.head()"
      ],
      "metadata": {
        "id": "FWlxHBZO6ECV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate support, confidence, lift, and more\n",
        "def calculate_metrics(rule, all_transactions):\n",
        "    antecedent = set(rule['transaction'])\n",
        "    consequent = set([rule['recommended_item']])\n",
        "\n",
        "    # Calculate support\n",
        "    support_antecedent = sum(1 for transaction in all_transactions if antecedent.issubset(set(transaction))) / len(all_transactions)\n",
        "    support_consequent = sum(1 for transaction in all_transactions if consequent.issubset(set(transaction))) / len(all_transactions)\n",
        "    support_rule = sum(1 for transaction in all_transactions if antecedent.issubset(set(transaction)) and consequent.issubset(set(transaction))) / len(all_transactions)\n",
        "\n",
        "    # Calculate confidence\n",
        "    confidence = support_rule / support_antecedent if support_antecedent != 0 else 0\n",
        "\n",
        "    # Calculate lift\n",
        "    lift = confidence / support_consequent if support_consequent != 0 else 0\n",
        "\n",
        "    # Calculate leverage\n",
        "    expected_support = support_antecedent * support_consequent\n",
        "    leverage = support_rule - expected_support\n",
        "\n",
        "    # Calculate conviction\n",
        "    conviction = (1 - support_consequent) / (1 - confidence) if confidence < 1 else np.inf\n",
        "\n",
        "    return {\n",
        "        'support_antecedent': support_antecedent,\n",
        "        'support_consequent': support_consequent,\n",
        "        'support_rule': support_rule,\n",
        "        'confidence': confidence,\n",
        "        'lift': lift,\n",
        "        'leverage': leverage,\n",
        "        'conviction': conviction\n",
        "    }\n",
        "\n",
        "# Calculate metrics for each rule\n",
        "metrics = []\n",
        "for _, rule in rules_df.iterrows():\n",
        "    rule_metrics = calculate_metrics(rule, transactions)\n",
        "    metrics.append(rule_metrics)\n",
        "\n",
        "# Add metrics to the DataFrame\n",
        "metrics_df = pd.DataFrame(metrics)\n",
        "\n",
        "# Combine the original rules with the calculated metrics\n",
        "df_rules = pd.concat([rules_df, metrics_df], axis=1)\n",
        "\n",
        "print(df_rules)"
      ],
      "metadata": {
        "id": "NHiRhE7LL1hJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sort the rules by average rating and select the top 100\n",
        "top_100_rules = df_rules.sort_values(by='conviction', ascending=False).head(100)\n",
        "\n",
        "# Display the top 100 rules\n",
        "print(top_100_rules[['lift', 'conviction']])"
      ],
      "metadata": {
        "id": "Qwi65LG_6DPz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(top_100_rules[['antecedents', 'consequents', 'lift', 'rating', 'confidence', 'support']])\n"
      ],
      "metadata": {
        "id": "Do-1UYX7zrZo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# perform item-based CF for rule mining\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def compute_item_ratings(invoice_items, item_sim_df):\n",
        "    \"\"\"\n",
        "    Calculate predicted rating for items not in the invoice.\n",
        "    Rating(Item X) = sum(sim(Item X, item_in_invoice)) / sum(similarities)\n",
        "    \"\"\"\n",
        "    scores = {}\n",
        "    norm_factor = 0\n",
        "\n",
        "    for item in invoice_items:\n",
        "        similar_items = item_sim_df[item].drop(index=invoice_items, errors=\"ignore\")  # Remove existing items\n",
        "        for sim_item, similarity in similar_items.items():\n",
        "            scores[sim_item] = scores.get(sim_item, 0) + similarity  # Aggregate similarity scores\n",
        "            norm_factor += similarity  # Normalize by sum of similarities\n",
        "\n",
        "    if norm_factor == 0:\n",
        "        return {}\n",
        "\n",
        "    # Normalize scores\n",
        "    for item in scores:\n",
        "        scores[item] /= norm_factor\n",
        "\n",
        "    return scores"
      ],
      "metadata": {
        "id": "cY8zXVcCWeJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.notebook import tqdm\n",
        "threshold = 0.2  # Min rating for an item to be considered\n",
        "\n",
        "rules = []\n",
        "for invoice in tqdm(df_onehot.index,desc=\"Processing Raiting\"):\n",
        "    existing_items = df_onehot.columns[df_onehot.loc[invoice] == 1].tolist()\n",
        "    predicted_ratings = compute_item_ratings(existing_items, item_sim_df)\n",
        "\n",
        "    # Select items with rating above threshold\n",
        "    recommended_items = {item for item, score in predicted_ratings.items() if score > threshold}\n",
        "\n",
        "    if recommended_items:\n",
        "        rules.append((set(existing_items), recommended_items))  # Format: {A, B, C} â†’ {D, F}\n",
        "\n",
        "rules_df = pd.DataFrame(rules, columns=[\"Antecedent\", \"Consequent\"])\n"
      ],
      "metadata": {
        "id": "iSw9rfrObaDR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compute Confidence and Lift\n",
        "def compute_rule_metrics(rules_df, df_onehot):\n",
        "    total_invoices = len(df_onehot)\n",
        "    metrics = []\n",
        "\n",
        "    for _, row in rules_df.iterrows():\n",
        "        antecedent = row[\"Antecedent\"]\n",
        "        consequent = row[\"Consequent\"]\n",
        "\n",
        "        support_A = (df_onehot[list(antecedent)].sum(axis=1) == len(antecedent)).mean()\n",
        "        support_B = (df_onehot[list(consequent)].sum(axis=1) == len(consequent)).mean()\n",
        "        support_A_B = ((df_onehot[list(antecedent)].sum(axis=1) == len(antecedent)) &\n",
        "                       (df_onehot[list(consequent)].sum(axis=1) == len(consequent))).mean()\n",
        "\n",
        "        confidence = support_A_B / support_A if support_A > 0 else 0\n",
        "        lift = confidence / support_B if support_B > 0 else 0\n",
        "        interestingness = confidence * lift  # Tradeoff metric\n",
        "\n",
        "        metrics.append((antecedent, consequent, confidence, lift, interestingness))\n",
        "\n",
        "    return pd.DataFrame(metrics, columns=[\"Antecedent\", \"Consequent\", \"Confidence\", \"Lift\", \"Interestingness\"])\n",
        "\n",
        "rules_eval_df = compute_rule_metrics(rules_df, df_onehot)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BhFRbIQYXfNu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select Top 100 Most Interesting Rules\n",
        "top_100_rules = rules_eval_df.sort_values(by=\"Interestingness\", ascending=False).head(100)\n",
        "print(top_100_rules)"
      ],
      "metadata": {
        "id": "1G8jxc4mXGN5"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}