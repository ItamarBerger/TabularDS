{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fX_NCznlNHye"
      },
      "source": [
        "# preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ItamarBerger/TabularDS"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pi_KucCvRDGx",
        "outputId": "03744c55-ec33-4590-aa08-aaa7da18631a"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'TabularDS' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load dataset\n",
        "import pandas as pd\n",
        "data = pd.read_csv('/content/TabularDS/final_project/code/data/online_retail.csv')\n",
        "# data = retails.copy()"
      ],
      "metadata": {
        "id": "dZwX40EVPMdy"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# retails.head()\n",
        "# print(len(retails))"
      ],
      "metadata": {
        "id": "zVGaG2KbQL92"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['InvoiceDate'] = pd.to_datetime(data['InvoiceDate'])\n",
        "data.dropna(subset=['InvoiceNo', 'Description'], inplace=True)\n",
        "\n",
        "transactions = data.groupby('InvoiceNo')['Description'].apply(list).tolist()"
      ],
      "metadata": {
        "id": "2tLlnwOvabf-"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transactions.head()"
      ],
      "metadata": {
        "id": "HKcBAD-Rdl5e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from mlxtend.frequent_patterns import apriori, association_rules\n",
        "from mlxtend.preprocessing import TransactionEncoder\n",
        "\n",
        "# Convert the transaction data into a one-hot encoded DataFrame\n",
        "te = TransactionEncoder()\n",
        "te_ary = te.fit(transactions).transform(transactions)\n",
        "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
        "\n",
        "# Apply Apriori to find frequent itemsets (min_support can be adjusted based on your needs)\n",
        "frequent_itemsets = apriori(df, min_support=0.05, use_colnames=True)\n",
        "\n",
        "# Generate association rules (min_threshold can be adjusted for confidence)\n",
        "rules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=0.0)\n",
        "\n",
        "# Display the results\n",
        "print(rules)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zrk63WVodLtC",
        "outputId": "e27b9390-3932-4de8-e482-f5676e0ec2e8"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Empty DataFrame\n",
            "Columns: [antecedents, consequents, antecedent support, consequent support, support, confidence, lift, representativity, leverage, conviction, zhangs_metric, jaccard, certainty, kulczynski]\n",
            "Index: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "yS_Pq9QcdKTy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# transforming raw text into IDs of items\n",
        "retails['DescriptionID'] = retails['Description'].astype('category').cat.codes\n",
        "\n",
        "# aggregating each transaction as set of items for each Invoice\n",
        "retails = retails.groupby('InvoiceNo')['DescriptionID'].apply(set).reset_index()\n",
        "# retails_with_ids.head()"
      ],
      "metadata": {
        "id": "ehDlc0NwSe9K",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 568
        },
        "outputId": "f5125677-041e-40e0-9506-1cd9daae256c"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'Description'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3805\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3806\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Description'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-56-0020063818d8>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# transforming raw text into IDs of items\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mretails\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'DescriptionID'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mretails\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Description'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'category'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# aggregating each transaction as set of items for each Invoice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mretails\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mretails\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'InvoiceNo'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'DescriptionID'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4102\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4104\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3810\u001b[0m             ):\n\u001b[1;32m   3811\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3812\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3813\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3814\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Description'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "retails.head()"
      ],
      "metadata": {
        "id": "4O67g1UrYb19"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from mlxtend.preprocessing import TransactionEncoder\n",
        "\n",
        "# Convert sets to lists\n",
        "transactions = retails[\"DescriptionID\"].apply(list).tolist()  # Convert sets to lists\n",
        "\n",
        "# Apply TransactionEncoder\n",
        "encoder = TransactionEncoder()\n",
        "encoded_array = encoder.fit(transactions).transform(transactions)\n",
        "\n",
        "# Convert to a DataFrame\n",
        "df_encoded = pd.DataFrame(encoded_array, columns=encoder.columns_)\n",
        "\n"
      ],
      "metadata": {
        "id": "vuKLswOtUPlq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_encoded.head())  # Check transformed DataFrame\n"
      ],
      "metadata": {
        "id": "4qgk5W9QWXo-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Apriori for rule mining"
      ],
      "metadata": {
        "id": "rU_fNB6EUTNP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from mlxtend.frequent_patterns import apriori, association_rules\n",
        "\n",
        "# Convert transactions to a DataFrame\n",
        "itemset = set(item for transaction in retails for item in transaction)\n",
        "data_dict = [{item: (item in transaction) for item in itemset} for transaction in retails]\n",
        "df = pd.DataFrame(data_dict)\n",
        "\n",
        "# Apply Apriori algorithm\n",
        "min_support = 0.5  # Minimum support threshold\n",
        "frequent_itemsets = apriori(df, min_support=min_support, use_colnames=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "RNYlCoQnUY89"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate association rules\n",
        "min_confidence = 0.03  # Minimum confidence threshold\n",
        "rules = association_rules(frequent_itemsets, metric='confidence', min_threshold=min_confidence)\n",
        "\n",
        "print(\"Frequent Itemsets:\")\n",
        "print(frequent_itemsets)\n",
        "print(\"\\nAssociation Rules:\")\n",
        "print(rules)"
      ],
      "metadata": {
        "id": "UrlyXiEYU8WA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter interesting rules\n",
        "interesting_rules = rules[(rules['confidence'] >= 0.8) & (rules['lift'] > 1.2) & (rules['support'])]\n",
        "print(\"\\nMost Interesting Association Rules:\")\n",
        "print(interesting_rules.head(100))\n"
      ],
      "metadata": {
        "id": "vhfwILeAWBzm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "item_sim_matrix = cosine_similarity(df_onehot.T)  # Item-item similarity\n",
        "item_sim_df = pd.DataFrame(item_sim_matrix, index=df_onehot.columns, columns=df_onehot.columns)\n"
      ],
      "metadata": {
        "id": "rK22Gt1sd1SM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# perform item-based CF for rule mining\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def compute_item_ratings(invoice_items, item_sim_df):\n",
        "    \"\"\"\n",
        "    Calculate predicted rating for items not in the invoice.\n",
        "    Rating(Item X) = sum(sim(Item X, item_in_invoice)) / sum(similarities)\n",
        "    \"\"\"\n",
        "    scores = {}\n",
        "    norm_factor = 0\n",
        "\n",
        "    for item in invoice_items:\n",
        "        similar_items = item_sim_df[item].drop(index=invoice_items, errors=\"ignore\")  # Remove existing items\n",
        "        for sim_item, similarity in similar_items.items():\n",
        "            scores[sim_item] = scores.get(sim_item, 0) + similarity  # Aggregate similarity scores\n",
        "            norm_factor += similarity  # Normalize by sum of similarities\n",
        "\n",
        "    if norm_factor == 0:\n",
        "        return {}\n",
        "\n",
        "    # Normalize scores\n",
        "    for item in scores:\n",
        "        scores[item] /= norm_factor\n",
        "\n",
        "    return scores"
      ],
      "metadata": {
        "id": "cY8zXVcCWeJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.notebook import tqdm\n",
        "threshold = 0.2  # Min rating for an item to be considered\n",
        "\n",
        "rules = []\n",
        "for invoice in tqdm(df_onehot.index,desc=\"Processing Raiting\"):\n",
        "    existing_items = df_onehot.columns[df_onehot.loc[invoice] == 1].tolist()\n",
        "    predicted_ratings = compute_item_ratings(existing_items, item_sim_df)\n",
        "\n",
        "    # Select items with rating above threshold\n",
        "    recommended_items = {item for item, score in predicted_ratings.items() if score > threshold}\n",
        "\n",
        "    if recommended_items:\n",
        "        rules.append((set(existing_items), recommended_items))  # Format: {A, B, C} â†’ {D, F}\n",
        "\n",
        "rules_df = pd.DataFrame(rules, columns=[\"Antecedent\", \"Consequent\"])\n"
      ],
      "metadata": {
        "id": "iSw9rfrObaDR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compute Confidence and Lift\n",
        "def compute_rule_metrics(rules_df, df_onehot):\n",
        "    total_invoices = len(df_onehot)\n",
        "    metrics = []\n",
        "\n",
        "    for _, row in rules_df.iterrows():\n",
        "        antecedent = row[\"Antecedent\"]\n",
        "        consequent = row[\"Consequent\"]\n",
        "\n",
        "        support_A = (df_onehot[list(antecedent)].sum(axis=1) == len(antecedent)).mean()\n",
        "        support_B = (df_onehot[list(consequent)].sum(axis=1) == len(consequent)).mean()\n",
        "        support_A_B = ((df_onehot[list(antecedent)].sum(axis=1) == len(antecedent)) &\n",
        "                       (df_onehot[list(consequent)].sum(axis=1) == len(consequent))).mean()\n",
        "\n",
        "        confidence = support_A_B / support_A if support_A > 0 else 0\n",
        "        lift = confidence / support_B if support_B > 0 else 0\n",
        "        interestingness = confidence * lift  # Tradeoff metric\n",
        "\n",
        "        metrics.append((antecedent, consequent, confidence, lift, interestingness))\n",
        "\n",
        "    return pd.DataFrame(metrics, columns=[\"Antecedent\", \"Consequent\", \"Confidence\", \"Lift\", \"Interestingness\"])\n",
        "\n",
        "rules_eval_df = compute_rule_metrics(rules_df, df_onehot)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BhFRbIQYXfNu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select Top 100 Most Interesting Rules\n",
        "top_100_rules = rules_eval_df.sort_values(by=\"Interestingness\", ascending=False).head(100)\n",
        "print(top_100_rules)"
      ],
      "metadata": {
        "id": "1G8jxc4mXGN5"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}