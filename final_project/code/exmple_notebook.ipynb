{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fX_NCznlNHye"
      },
      "source": [
        "# preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ItamarBerger/TabularDS"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pi_KucCvRDGx",
        "outputId": "29dbe247-e8de-4bf2-ed44-4a2506821037"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'TabularDS' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load dataset\n",
        "import pandas as pd\n",
        "data = pd.read_csv('/content/TabularDS/final_project/code/data/online_retail.csv')\n",
        "# data = retails.copy()"
      ],
      "metadata": {
        "id": "dZwX40EVPMdy"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# retails.head()\n",
        "# print(len(retails))\n",
        "data.head()"
      ],
      "metadata": {
        "id": "zVGaG2KbQL92"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Apriori for rule mining"
      ],
      "metadata": {
        "id": "rU_fNB6EUTNP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data['InvoiceDate'] = pd.to_datetime(data['InvoiceDate'])\n",
        "data.dropna(subset=['InvoiceNo', 'Description'], inplace=True)\n",
        "\n",
        "transactions = data.groupby('CustomerID')['Description'].apply(list).tolist()\n",
        "print(f\"number of transactions is: {len(transactions)}\")"
      ],
      "metadata": {
        "id": "2tLlnwOvabf-",
        "outputId": "d7ed48ea-e29b-4619-bf04-b9bb406fa8b5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of transactions is: 4372\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install efficient-apriori"
      ],
      "metadata": {
        "id": "U3eNxbjikpNe",
        "outputId": "c9129fb5-64b5-4156-e0e9-8397533c51d3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: efficient-apriori in /usr/local/lib/python3.11/dist-packages (2.0.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# from mlxtend.frequent_patterns import apriori, association_rules\n",
        "from efficient_apriori import apriori\n",
        "from mlxtend.preprocessing import TransactionEncoder\n",
        "\n",
        "# Sample a smaller subset\n",
        "transactions = transactions\n",
        "\n",
        "# Optimize TransactionEncoder using sparse format\n",
        "te = TransactionEncoder()\n",
        "te_ary = te.fit(transactions).transform(transactions, sparse=True)\n",
        "df = pd.DataFrame.sparse.from_spmatrix(te_ary, columns=te.columns_)\n",
        "\n",
        "# Apply Apriori with a higher support threshold (adjust as needed)\n",
        "# frequent_itemsets, rules = apriori(df, min_support=0.05, min_confidence = 0.5)\n",
        "from mlxtend.frequent_patterns import fpgrowth, association_rules\n",
        "frequent_itemsets = fpgrowth(df, min_support=0.05, use_colnames=True)\n",
        "rules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=1.0)\n",
        "\n",
        "# Generate association rules\n",
        "# if not frequent_itemsets.empty:\n",
        "#     rules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=1.0)\n",
        "print(rules)"
      ],
      "metadata": {
        "id": "Zrk63WVodLtC",
        "outputId": "b38f8196-c11d-4724-d66b-15ba66958d6c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-64-51d913d0addf>:12: FutureWarning: Allowing arbitrary scalar fill_value in SparseDtype is deprecated. In a future version, the fill_value must be a valid value for the SparseDtype.subtype.\n",
            "  df = pd.DataFrame.sparse.from_spmatrix(te_ary, columns=te.columns_)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                               antecedents  \\\n",
            "0       (ROSES REGENCY TEACUP AND SAUCER )   \n",
            "1               (REGENCY CAKESTAND 3 TIER)   \n",
            "2        (PACK OF 72 RETROSPOT CAKE CASES)   \n",
            "3            (60 TEATIME FAIRY CAKE CASES)   \n",
            "4       (ROSES REGENCY TEACUP AND SAUCER )   \n",
            "..                                     ...   \n",
            "215   (WHITE HANGING HEART T-LIGHT HOLDER)   \n",
            "216   (WOODEN STAR CHRISTMAS SCANDINAVIAN)   \n",
            "217  (WOODEN HEART CHRISTMAS SCANDINAVIAN)   \n",
            "218          (LARGE WHITE HEART OF WICKER)   \n",
            "219          (SMALL WHITE HEART OF WICKER)   \n",
            "\n",
            "                               consequents  antecedent support  \\\n",
            "0               (REGENCY CAKESTAND 3 TIER)            0.097896   \n",
            "1       (ROSES REGENCY TEACUP AND SAUCER )            0.203111   \n",
            "2            (60 TEATIME FAIRY CAKE CASES)            0.145700   \n",
            "3        (PACK OF 72 RETROSPOT CAKE CASES)            0.095151   \n",
            "4        (GREEN REGENCY TEACUP AND SAUCER)            0.097896   \n",
            "..                                     ...                 ...   \n",
            "215                (HEART OF WICKER LARGE)            0.196478   \n",
            "216  (WOODEN HEART CHRISTMAS SCANDINAVIAN)            0.076624   \n",
            "217   (WOODEN STAR CHRISTMAS SCANDINAVIAN)            0.078911   \n",
            "218          (SMALL WHITE HEART OF WICKER)            0.080284   \n",
            "219          (LARGE WHITE HEART OF WICKER)            0.086688   \n",
            "\n",
            "     consequent support   support  confidence      lift  representativity  \\\n",
            "0              0.203111  0.074337    0.759346  3.738581               1.0   \n",
            "1              0.097896  0.074337    0.365991  3.738581               1.0   \n",
            "2              0.095151  0.061985    0.425432  4.471124               1.0   \n",
            "3              0.145700  0.061985    0.651442  4.471124               1.0   \n",
            "4              0.089433  0.074337    0.759346  8.490690               1.0   \n",
            "..                  ...       ...         ...       ...               ...   \n",
            "215            0.111619  0.061070    0.310827  2.784700               1.0   \n",
            "216            0.078911  0.057868    0.755224  9.570547               1.0   \n",
            "217            0.076624  0.057868    0.733333  9.570547               1.0   \n",
            "218            0.086688  0.051006    0.635328  7.328898               1.0   \n",
            "219            0.080284  0.051006    0.588391  7.328898               1.0   \n",
            "\n",
            "     leverage  conviction  zhangs_metric   jaccard  certainty  kulczynski  \n",
            "0    0.054453    3.311346       0.812011  0.327952   0.698008    0.562668  \n",
            "1    0.054453    1.422857       0.919223  0.327952   0.297189    0.562668  \n",
            "2    0.048122    1.574833       0.908747  0.346547   0.365012    0.538437  \n",
            "3    0.048122    2.450958       0.857980  0.346547   0.591996    0.538437  \n",
            "4    0.065582    3.783716       0.977962  0.657895   0.735710    0.795274  \n",
            "..        ...         ...            ...       ...        ...         ...  \n",
            "215  0.039140    1.289052       0.797607  0.247222   0.224236    0.428979  \n",
            "216  0.051822    3.762985       0.969825  0.592506   0.734253    0.744279  \n",
            "217  0.051822    3.462660       0.972233  0.592506   0.711205    0.744279  \n",
            "218  0.044047    2.504473       0.938935  0.439842   0.600714    0.611859  \n",
            "219  0.044047    2.234439       0.945519  0.439842   0.552460    0.611859  \n",
            "\n",
            "[220 rows x 14 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# top rules by lift\n",
        "# top_lift_rules = rules.sort_values('lift', ascending=False).head(100)\n",
        "# print(top_lift_rules)\n"
      ],
      "metadata": {
        "id": "Mch0QcYun0r-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "# import networkx as nx\n",
        "# from mlxtend.frequent_patterns import apriori, association_rules\n",
        "\n",
        "# # Create a directed graph\n",
        "# G = nx.DiGraph()\n",
        "# narrow_rules = rules[0:25]\n",
        "# # Add nodes and directed edges\n",
        "# for _, rule in narrow_rules.iterrows():\n",
        "#     antecedent = ', '.join(list(rule['antecedents']))\n",
        "#     consequent = ', '.join(list(rule['consequents']))\n",
        "\n",
        "#     # Add a directed edge from antecedent to consequent\n",
        "#     G.add_edge(antecedent, consequent, weight=rule['lift'], support=rule['support'], confidence=rule['confidence'])\n",
        "\n",
        "# # Draw the directed graph\n",
        "# plt.figure(figsize=(12, 12))\n",
        "# pos = nx.spring_layout(G, k=0.3, iterations=20)  # Layout for the graph\n",
        "\n",
        "# # Draw the nodes and edges\n",
        "# nx.draw_networkx_nodes(G, pos, node_size=200, node_color='lightblue', alpha=0.7)\n",
        "# nx.draw_networkx_edges(G, pos, width=2.0, alpha=0.5, arrowstyle='->', arrowsize=10)\n",
        "# nx.draw_networkx_labels(G, pos, font_size=10, font_color='black')\n",
        "\n",
        "# # Add edge labels (optional, if you want to display lift, confidence, or support)\n",
        "# # edge_labels = nx.get_edge_attributes(G, 'weight')\n",
        "# # nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels)\n",
        "\n",
        "# plt.title(\"Association Rules Directed Graph\")\n",
        "# plt.axis('off')\n",
        "# plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "4pHXBq27f-Lb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import plotly.graph_objects as go\n",
        "# import networkx as nx\n",
        "# from mlxtend.frequent_patterns import apriori, association_rules\n",
        "\n",
        "# # Example association rules (you need to load your own rules here)\n",
        "# # Example: rules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=1.0)\n",
        "\n",
        "# # Create a directed graph\n",
        "# G = nx.DiGraph()\n",
        "\n",
        "# # Add nodes and directed edges\n",
        "# for _, rule in narrow_rules.iterrows():\n",
        "#     antecedent = ', '.join(list(rule['antecedents']))\n",
        "#     consequent = ', '.join(list(rule['consequents']))\n",
        "\n",
        "#     # Add a directed edge from antecedent to consequent\n",
        "#     G.add_edge(antecedent, consequent, weight=rule['lift'], support=rule['support'], confidence=rule['confidence'])\n",
        "\n",
        "# # Get positions of nodes using a layout\n",
        "# pos = nx.spring_layout(G, k=0.3, iterations=20)\n",
        "\n",
        "# # Get edge and node information for Plotly\n",
        "# edge_x = []\n",
        "# edge_y = []\n",
        "# for edge in G.edges():\n",
        "#     x0, y0 = pos[edge[0]]\n",
        "#     x1, y1 = pos[edge[1]]\n",
        "#     edge_x.append(x0)\n",
        "#     edge_y.append(y0)\n",
        "#     edge_x.append(x1)\n",
        "#     edge_y.append(y1)\n",
        "\n",
        "# # Node positions\n",
        "# node_x = [pos[node][0] for node in G.nodes()]\n",
        "# node_y = [pos[node][1] for node in G.nodes()]\n",
        "\n",
        "# # Create hover text for nodes (empty text for default)\n",
        "# hover_text = [node for node in G.nodes()]\n",
        "\n",
        "# # Create the plotly figure\n",
        "# fig = go.Figure()\n",
        "\n",
        "# # Add directed edges with arrows (using annotations for arrows)\n",
        "# for edge in G.edges():\n",
        "#     x0, y0 = pos[edge[0]]\n",
        "#     x1, y1 = pos[edge[1]]\n",
        "\n",
        "#     # Add edge line\n",
        "#     fig.add_trace(go.Scatter(x=[x0, x1], y=[y0, y1],\n",
        "#                              line=dict(width=1, color='gray'),\n",
        "#                              mode='lines',\n",
        "#                              showlegend=False))\n",
        "\n",
        "#     # Add arrow (using annotations)\n",
        "#     fig.add_annotation(\n",
        "#         x=x1, y=y1,\n",
        "#         ax=x0, ay=y0,\n",
        "#         axref=\"x\", ayref=\"y\", xref=\"x\", yref=\"y\",\n",
        "#         showarrow=True, arrowhead=2, arrowsize=1,\n",
        "#         arrowcolor='gray', opacity=0.7\n",
        "#     )\n",
        "\n",
        "# # Add nodes to the plot\n",
        "# fig.add_trace(go.Scatter(x=node_x, y=node_y,\n",
        "#                          mode='markers',\n",
        "#                          hoverinfo='text',  # This ensures text shows on hover only\n",
        "#                          hovertext=hover_text,  # Set hovertext to show item names on hover\n",
        "#                          marker=dict(color='skyblue', size=10, line=dict(width=1, color='black'))))\n",
        "\n",
        "# # Add edge labels (optional: you can use lift or other metrics)\n",
        "# edge_labels = nx.get_edge_attributes(G, 'weight')\n",
        "# for edge, label in edge_labels.items():\n",
        "#     x0, y0 = pos[edge[0]]\n",
        "#     x1, y1 = pos[edge[1]]\n",
        "#     fig.add_trace(go.Scatter(x=[(x0+x1)/2], y=[(y0+y1)/2],\n",
        "#                              text=[f'Lift: {label:.2f}'],\n",
        "#                              mode='text',\n",
        "#                              showlegend=False))\n",
        "\n",
        "# # Update layout settings for better visualization\n",
        "# fig.update_layout(title=\"Association Rules Directed Graph (Interactive)\",\n",
        "#                   showlegend=False,\n",
        "#                   hovermode='closest',\n",
        "#                   xaxis=dict(showgrid=False, zeroline=False),\n",
        "#                   yaxis=dict(showgrid=False, zeroline=False))\n",
        "\n",
        "# # Show the plot\n",
        "# fig.show()\n"
      ],
      "metadata": {
        "id": "IJrqXXbgiEl0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import seaborn as sns\n",
        "\n",
        "# # Plot support vs confidence\n",
        "# plt.figure(figsize=(10, 6))\n",
        "# sns.scatterplot(x='support', y='confidence', data=rules, hue='lift', palette='coolwarm', size='lift', sizes=(50, 300))\n",
        "# plt.title(\"Support vs Confidence with Lift\")\n",
        "# plt.xlabel('Support')\n",
        "# plt.ylabel('Confidence')\n",
        "# plt.show()\n",
        "\n",
        "# # Plot lift distribution\n",
        "# plt.figure(figsize=(10, 6))\n",
        "# sns.histplot(rules['lift'], kde=True)\n",
        "# plt.title(\"Lift Distribution\")\n",
        "# plt.xlabel('Lift')\n",
        "# plt.ylabel('Frequency')\n",
        "# plt.show()\n"
      ],
      "metadata": {
        "id": "p1uXodl4gAMw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cluster items + Apriori\n"
      ],
      "metadata": {
        "id": "-qqhjLNWXnsL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "# from mlxtend.frequent_patterns import apriori, association_rules\n",
        "from sklearn.cluster import KMeans\n",
        "from scipy.spatial.distance import cdist\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from mlxtend.preprocessing import TransactionEncoder\n",
        "from sklearn.cluster import KMeans\n",
        "from scipy.spatial.distance import cdist\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#  Convert Transactions to One-Hot Encoded Format\n",
        "te = TransactionEncoder()\n",
        "one_hot = te.fit_transform(transactions)\n",
        "df = pd.DataFrame(one_hot, columns=te.columns_)\n",
        "\n",
        "#  Find Optimal Number of Clusters (Elbow Method)\n",
        "# distortions = []\n",
        "# K_range = range(2, 10)  # Try different values of K\n",
        "\n",
        "# for k in K_range:\n",
        "#     kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "#     kmeans.fit(df)\n",
        "#     distortions.append(sum(np.min(cdist(df, kmeans.cluster_centers_, 'euclidean'), axis=1)) / df.shape[0])\n",
        "\n",
        "# #  Plot the Elbow Curve\n",
        "# plt.figure(figsize=(8, 5))\n",
        "# plt.plot(K_range, distortions, marker='o', linestyle='--')\n",
        "# plt.xlabel('Number of Clusters (k)')\n",
        "# plt.ylabel('Distortion')\n",
        "# plt.title('Elbow Method for Optimal k')\n",
        "# plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zJds2I7wXq9T"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Perform K-Means with Optimal k\n",
        "optimal_k = 4  # Choose based on the Elbow Method\n",
        "kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
        "df['Cluster'] = kmeans.fit_predict(df)\n",
        "\n",
        "#  Print Cluster Assignments\n",
        "print(df[['Cluster']])\n",
        "# for i in range(optimal_k):\n",
        "# # Print Cluster Assignments\n",
        "# print(df[['Cluster']])\n",
        "\n",
        "# Iterate over clusters and print the sum of values for each cluster\n"
      ],
      "metadata": {
        "id": "f_ZRBymlzjhC",
        "outputId": "f4f5e35b-c6f6-4329-92e0-64761f8d2715",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Cluster\n",
            "0           3\n",
            "1           2\n",
            "2           3\n",
            "3           2\n",
            "4           3\n",
            "...       ...\n",
            "4367        3\n",
            "4368        3\n",
            "4369        3\n",
            "4370        0\n",
            "4371        3\n",
            "\n",
            "[4372 rows x 1 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df['Cluster'].value_counts().sort_index())\n"
      ],
      "metadata": {
        "id": "ppwiScnU03tS",
        "outputId": "046a281d-eed2-453d-eee2-6656b614b2b4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cluster\n",
            "0     160\n",
            "1       5\n",
            "2     962\n",
            "3    3245\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from efficient_apriori import apriori\n",
        "\n",
        "# Function to process a single cluster\n",
        "def process_cluster(cluster_id, df):\n",
        "    print(f\"\\nProcessing Cluster {cluster_id}...\")\n",
        "\n",
        "    # Extract transactions for the cluster\n",
        "    cluster_data = df[df['Cluster'] == cluster_id].drop(columns=['Cluster'])\n",
        "\n",
        "\n",
        "    # Run Apriori\n",
        "    from mlxtend.frequent_patterns import fpgrowth, association_rules\n",
        "    # from scipy.sparse import csr_matrix\n",
        "    # cluster_data_sparse = csr_matrix(cluster_data)\n",
        "    # df_cluster_data = pd.DataFrame.sparse.from_spmatrix(cluster_data_sparse, columns=cluster_data.columns)\n",
        "\n",
        "    scale_k = 0.05 * np.sqrt(len(transactions))\n",
        "    min_support =  scale_k / np.sqrt(len(cluster_data))\n",
        "    print(f\"min_support: {min_support}\")\n",
        "\n",
        "\n",
        "    frequent_itemsets = fpgrowth(cluster_data, min_support=min_support, use_colnames=True)\n",
        "    if frequent_itemsets.empty:\n",
        "        print(f\"No frequent itemsets found in Cluster {cluster_id}.\")\n",
        "        return None\n",
        "    rules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=1.0)\n",
        "\n",
        "    print(f\"Cluster {cluster_id}: {len(rules)} rules generated.\")\n",
        "\n",
        "    return cluster_id, {\"frequent_itemsets\": frequent_itemsets, \"rules\": rules}\n",
        "\n",
        "# Process clusters sequentially (without parallel computation)\n",
        "cluster_results = {}\n",
        "# Loop through all unique cluster IDs\n",
        "for cluster_id in sorted(df['Cluster'].unique()):\n",
        "    if df['Cluster'].value_counts()[cluster_id] < 30:\n",
        "        continue\n",
        "    cluster_results[cluster_id] = process_cluster(cluster_id, df)\n"
      ],
      "metadata": {
        "id": "dOb8TpaDX2Cg",
        "outputId": "1e7a3437-8e8f-4c71-e9f4-6d75b270091e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing Cluster 0...\n",
            "min_support: 0.26136660077370255\n",
            "Cluster 0: 197186 rules generated.\n",
            "\n",
            "Processing Cluster 2...\n",
            "min_support: 0.10659149291452093\n",
            "Cluster 2: 314 rules generated.\n",
            "\n",
            "Processing Cluster 3...\n",
            "min_support: 0.058036702695658755\n",
            "Cluster 3: 0 rules generated.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Initialize dictionary to store average metrics per cluster\n",
        "cluster_metrics = {}\n",
        "\n",
        "for cluster_id, data in cluster_results.items():\n",
        "    rules = data[\"rules\"]\n",
        "\n",
        "    if not rules.empty:\n",
        "        avg_support = rules[\"support\"].mean()\n",
        "        avg_confidence = rules[\"confidence\"].mean()\n",
        "        avg_lift = rules[\"lift\"].mean()\n",
        "    else:\n",
        "        avg_support, avg_confidence, avg_lift = 0, 0, 0  # Handle empty clusters\n",
        "\n",
        "    cluster_metrics[cluster_id] = {\n",
        "        \"Avg Support\": avg_support,\n",
        "        \"Avg Confidence\": avg_confidence,\n",
        "        \"Avg Lift\": avg_lift\n",
        "    }\n",
        "\n",
        "# Convert dictionary to DataFrame for easier plotting\n",
        "metrics_df = pd.DataFrame.from_dict(cluster_metrics, orient=\"index\")\n",
        "print(metrics_df)\n"
      ],
      "metadata": {
        "id": "HEVQidHkX6ft",
        "outputId": "a9a20d76-c3cb-44c1-aee3-22a815ec6070",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "tuple indices must be integers or slices, not str",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-73-7dc60b517dc0>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcluster_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcluster_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mrules\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"rules\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mrules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: tuple indices must be integers or slices, not str"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using item-based CF for rules mining"
      ],
      "metadata": {
        "id": "T2N5TnA2kZdm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "yS_Pq9QcdKTy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.spatial.distance import pdist, squareform\n",
        "\n",
        "transactions = transactions\n",
        "\n",
        "\n",
        "def calculate_jaccard_similarity(transaction_matrix):\n",
        "    \"\"\"Calculate Jaccard similarity between all item pairs\"\"\"\n",
        "    binary_matrix = (transaction_matrix > 0).astype(int)\n",
        "    jaccard_dist = pdist(binary_matrix.T, metric='jaccard')\n",
        "    jaccard_sim = 1 - squareform(jaccard_dist)\n",
        "    return pd.DataFrame(jaccard_sim, index=transaction_matrix.columns, columns=transaction_matrix.columns)\n",
        "\n",
        "def calculate_item_means(transaction_matrix):\n",
        "    \"\"\"Calculate mean occurrence (probability) for each item\"\"\"\n",
        "    return transaction_matrix.mean()\n",
        "\n",
        "def calculate_recommendation_score(transaction_items, candidate_item, item_similarity_df, transaction_matrix):\n",
        "    \"\"\"\n",
        "    Calculate recommendation score using adjusted formula for binary ratings:\n",
        "    score(u,i) = Σj similarity(i,j)(r(u,j) - r̄j) / Σj |similarity(i,j)| + r̄i\n",
        "    where:\n",
        "    - r(u,j) is 1 if item j is in transaction, 0 otherwise\n",
        "    - r̄j is the mean occurrence of item j across all transactions\n",
        "    - r̄i is the mean occurrence of candidate item i\n",
        "    \"\"\"\n",
        "    # Get mean occurrences for all items\n",
        "    item_means = calculate_item_means(transaction_matrix)\n",
        "\n",
        "    numerator = 0\n",
        "    denominator = 0\n",
        "\n",
        "    # Calculate the weighted sum of deviations from mean\n",
        "    for item in transaction_items:\n",
        "        similarity = item_similarity_df.loc[candidate_item, item]\n",
        "        # r(u,j) is 1 since item is in transaction\n",
        "        rating_deviation = 1 - item_means[item]  # (r(u,j) - r̄j)\n",
        "        numerator += similarity * rating_deviation\n",
        "        denominator += abs(similarity)\n",
        "\n",
        "    # Add the candidate item's mean occurrence\n",
        "    mean_i = item_means[candidate_item]\n",
        "\n",
        "    if denominator != 0:\n",
        "        score = (numerator / denominator) + mean_i\n",
        "    else:\n",
        "        score = mean_i\n",
        "\n",
        "    return score\n",
        "\n",
        "def generate_transaction_rules(transactions):\n",
        "    # Create binary transaction matrix\n",
        "    item_list = sorted(set([item for sublist in transactions for item in sublist]))\n",
        "    transaction_matrix = pd.DataFrame(0, index=range(len(transactions)), columns=item_list)\n",
        "    for idx, transaction in enumerate(transactions):\n",
        "        for item in transaction:\n",
        "            transaction_matrix.at[idx, item] = 1\n",
        "\n",
        "    # Calculate item similarity\n",
        "    item_similarity_df = calculate_jaccard_similarity(transaction_matrix)\n",
        "\n",
        "    # Generate recommendations\n",
        "    recommendations = []\n",
        "\n",
        "    for transaction in transactions:\n",
        "        scores = {}\n",
        "\n",
        "        # Calculate scores for all possible items not in transaction\n",
        "        for candidate_item in item_list:\n",
        "            if candidate_item not in transaction:\n",
        "                score = calculate_recommendation_score(\n",
        "                    transaction,\n",
        "                    candidate_item,\n",
        "                    item_similarity_df,\n",
        "                    transaction_matrix\n",
        "                )\n",
        "                scores[candidate_item] = score\n",
        "\n",
        "        if scores:\n",
        "            # Get best recommendation\n",
        "            best_item = max(scores.items(), key=lambda x: x[1])\n",
        "\n",
        "            recommendation = {\n",
        "                'transaction': transaction,\n",
        "                'recommended_item': best_item[0],\n",
        "                'score': best_item[1]\n",
        "            }\n",
        "            recommendations.append(recommendation)\n",
        "\n",
        "    return pd.DataFrame(recommendations)\n",
        "\n",
        "\n",
        "# Usage\n",
        "rules_df = generate_transaction_rules(transactions)\n",
        "\n"
      ],
      "metadata": {
        "id": "rK22Gt1sd1SM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rules_df.head()"
      ],
      "metadata": {
        "id": "FWlxHBZO6ECV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate support, confidence, lift, and more\n",
        "def calculate_metrics(rule, all_transactions):\n",
        "    antecedent = set(rule['transaction'])\n",
        "    consequent = set([rule['recommended_item']])\n",
        "\n",
        "    # Calculate support\n",
        "    support_antecedent = sum(1 for transaction in all_transactions if antecedent.issubset(set(transaction))) / len(all_transactions)\n",
        "    support_consequent = sum(1 for transaction in all_transactions if consequent.issubset(set(transaction))) / len(all_transactions)\n",
        "    support_rule = sum(1 for transaction in all_transactions if antecedent.issubset(set(transaction)) and consequent.issubset(set(transaction))) / len(all_transactions)\n",
        "\n",
        "    # Calculate confidence\n",
        "    confidence = support_rule / support_antecedent if support_antecedent != 0 else 0\n",
        "\n",
        "    # Calculate lift\n",
        "    lift = confidence / support_consequent if support_consequent != 0 else 0\n",
        "\n",
        "    # Calculate leverage\n",
        "    expected_support = support_antecedent * support_consequent\n",
        "    leverage = support_rule - expected_support\n",
        "\n",
        "    # Calculate conviction\n",
        "    conviction = (1 - support_consequent) / (1 - confidence) if confidence < 1 else np.inf\n",
        "\n",
        "    return {\n",
        "        'support_antecedent': support_antecedent,\n",
        "        'support_consequent': support_consequent,\n",
        "        'support_rule': support_rule,\n",
        "        'confidence': confidence,\n",
        "        'lift': lift,\n",
        "        'leverage': leverage,\n",
        "        'conviction': conviction\n",
        "    }\n",
        "\n",
        "# Calculate metrics for each rule\n",
        "metrics = []\n",
        "for _, rule in rules_df.iterrows():\n",
        "    rule_metrics = calculate_metrics(rule, transactions)\n",
        "    metrics.append(rule_metrics)\n",
        "\n",
        "# Add metrics to the DataFrame\n",
        "metrics_df = pd.DataFrame(metrics)\n",
        "\n",
        "# Combine the original rules with the calculated metrics\n",
        "df_rules = pd.concat([rules_df, metrics_df], axis=1)\n",
        "\n",
        "print(df_rules)"
      ],
      "metadata": {
        "id": "NHiRhE7LL1hJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sort the rules by average rating and select the top 100\n",
        "top_100_rules = df_rules.sort_values(by='conviction', ascending=False).head(100)\n",
        "\n",
        "# Display the top 100 rules\n",
        "print(top_100_rules[['lift', 'conviction']])"
      ],
      "metadata": {
        "id": "Qwi65LG_6DPz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(top_100_rules[['antecedents', 'consequents', 'lift', 'rating', 'confidence', 'support']])\n"
      ],
      "metadata": {
        "id": "Do-1UYX7zrZo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# perform item-based CF for rule mining\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def compute_item_ratings(invoice_items, item_sim_df):\n",
        "    \"\"\"\n",
        "    Calculate predicted rating for items not in the invoice.\n",
        "    Rating(Item X) = sum(sim(Item X, item_in_invoice)) / sum(similarities)\n",
        "    \"\"\"\n",
        "    scores = {}\n",
        "    norm_factor = 0\n",
        "\n",
        "    for item in invoice_items:\n",
        "        similar_items = item_sim_df[item].drop(index=invoice_items, errors=\"ignore\")  # Remove existing items\n",
        "        for sim_item, similarity in similar_items.items():\n",
        "            scores[sim_item] = scores.get(sim_item, 0) + similarity  # Aggregate similarity scores\n",
        "            norm_factor += similarity  # Normalize by sum of similarities\n",
        "\n",
        "    if norm_factor == 0:\n",
        "        return {}\n",
        "\n",
        "    # Normalize scores\n",
        "    for item in scores:\n",
        "        scores[item] /= norm_factor\n",
        "\n",
        "    return scores"
      ],
      "metadata": {
        "id": "cY8zXVcCWeJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.notebook import tqdm\n",
        "threshold = 0.2  # Min rating for an item to be considered\n",
        "\n",
        "rules = []\n",
        "for invoice in tqdm(df_onehot.index,desc=\"Processing Raiting\"):\n",
        "    existing_items = df_onehot.columns[df_onehot.loc[invoice] == 1].tolist()\n",
        "    predicted_ratings = compute_item_ratings(existing_items, item_sim_df)\n",
        "\n",
        "    # Select items with rating above threshold\n",
        "    recommended_items = {item for item, score in predicted_ratings.items() if score > threshold}\n",
        "\n",
        "    if recommended_items:\n",
        "        rules.append((set(existing_items), recommended_items))  # Format: {A, B, C} → {D, F}\n",
        "\n",
        "rules_df = pd.DataFrame(rules, columns=[\"Antecedent\", \"Consequent\"])\n"
      ],
      "metadata": {
        "id": "iSw9rfrObaDR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compute Confidence and Lift\n",
        "def compute_rule_metrics(rules_df, df_onehot):\n",
        "    total_invoices = len(df_onehot)\n",
        "    metrics = []\n",
        "\n",
        "    for _, row in rules_df.iterrows():\n",
        "        antecedent = row[\"Antecedent\"]\n",
        "        consequent = row[\"Consequent\"]\n",
        "\n",
        "        support_A = (df_onehot[list(antecedent)].sum(axis=1) == len(antecedent)).mean()\n",
        "        support_B = (df_onehot[list(consequent)].sum(axis=1) == len(consequent)).mean()\n",
        "        support_A_B = ((df_onehot[list(antecedent)].sum(axis=1) == len(antecedent)) &\n",
        "                       (df_onehot[list(consequent)].sum(axis=1) == len(consequent))).mean()\n",
        "\n",
        "        confidence = support_A_B / support_A if support_A > 0 else 0\n",
        "        lift = confidence / support_B if support_B > 0 else 0\n",
        "        interestingness = confidence * lift  # Tradeoff metric\n",
        "\n",
        "        metrics.append((antecedent, consequent, confidence, lift, interestingness))\n",
        "\n",
        "    return pd.DataFrame(metrics, columns=[\"Antecedent\", \"Consequent\", \"Confidence\", \"Lift\", \"Interestingness\"])\n",
        "\n",
        "rules_eval_df = compute_rule_metrics(rules_df, df_onehot)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BhFRbIQYXfNu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select Top 100 Most Interesting Rules\n",
        "top_100_rules = rules_eval_df.sort_values(by=\"Interestingness\", ascending=False).head(100)\n",
        "print(top_100_rules)"
      ],
      "metadata": {
        "id": "1G8jxc4mXGN5"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}