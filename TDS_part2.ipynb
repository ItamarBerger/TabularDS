{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP35sWzYoZ+uyEgxnCAHcTe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ItamarBerger/TabularDS-part1/blob/main/TDS_part2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3P83jGQMtaup",
        "outputId": "3deb42b2-fdff-499d-9d78-e617a8185e99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'TabularDS-part1'...\n",
            "remote: Enumerating objects: 87, done.\u001b[K\n",
            "remote: Counting objects: 100% (87/87), done.\u001b[K\n",
            "remote: Compressing objects: 100% (77/77), done.\u001b[K\n",
            "remote: Total 87 (delta 14), reused 20 (delta 3), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (87/87), 5.97 MiB | 14.42 MiB/s, done.\n",
            "Resolving deltas: 100% (14/14), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/ItamarBerger/TabularDS-part1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load part-1 baseline model"
      ],
      "metadata": {
        "id": "w6r6KIyJmTGV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "\n",
        "# Load the saved model\n",
        "model = joblib.load('/content/TabularDS-part1/models/part1_model.pkl')\n",
        "\n",
        "# Load the same dataset to ensure consistency\n",
        "dtf = pd.read_csv('/content/TabularDS-part1/data/avocado.csv')"
      ],
      "metadata": {
        "id": "-r3UkxAFyUGW"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Error anaylasis and work plan\n",
        "## intro:\n",
        "As we recall, our baseline from part-1 already had pretty solid performance (r2 , MSE, residuals, and propotional residuals).\n",
        "Our goal in this notebook is to present creative approches for improving the baseline model\n",
        "\n",
        "## Error Analysis Conclusions\n",
        "1. We observed a sub-optimal RMSE score (around 0.11) in the baseline model.\n",
        "2. Specifically noticed a model's tendency to underestimate\n",
        "3. No difference in Avocado type in model's prediction error\n",
        "4. High importance for the Date feature, which we didn't take into account last part. in this part we'll adress this issue.\n",
        "5. Many features had skewed distribution or outliers on part-1\n",
        "\n",
        "## Work-plan\n",
        "0. Identify weaknesses in the initial model (some of them are listed here below)\n",
        "1. Feature engeeniring\n",
        "   - tramnsorming the **Date feature** into something more meaningful to help the model learn the relation we saw on part-1 between the date and the average avocado price\n",
        "\n",
        "   - Adding **hidden feature** to complete totalVolume as sum of 3 different avocado volumes by sizes and a hidden amount of avocados not from those 3 categories.\n",
        "2. Handling **outliers** more affectively than in part-1\n",
        "3. Handling ** skewed features** using IQR"
      ],
      "metadata": {
        "id": "YiUN3tREmejZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Basic pre-processing and cleaning"
      ],
      "metadata": {
        "id": "_kbqzR3CvIjv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(dtf.isnull().sum())\n",
        "dtf.drop('Unnamed: 0')"
      ],
      "metadata": {
        "id": "ZD9_hBt6vMzr",
        "outputId": "7515f946-9d93-49ce-bfb7-a431573aaa23",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unnamed: 0      0\n",
            "Date            0\n",
            "AveragePrice    0\n",
            "Total Volume    0\n",
            "4046            0\n",
            "4225            0\n",
            "4770            0\n",
            "Total Bags      0\n",
            "Small Bags      0\n",
            "Large Bags      0\n",
            "XLarge Bags     0\n",
            "type            0\n",
            "year            0\n",
            "region          0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "dtf['type'] = dtf['type'].map({'organic': 1, 'conventional': 0})\n",
        "le = LabelEncoder()\n",
        "\n",
        "# Apply label encoding to the 'region' column\n",
        "dtf['region'] = le.fit_transform(dtf['region'])"
      ],
      "metadata": {
        "id": "hYmCr7vEueNU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Let's transform the Date feature into its core logical components (year, month, week)"
      ],
      "metadata": {
        "id": "veuxgKhOu_y5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dtf.columns = dtf.columns.str.replace(' ', '')\n",
        "dtf['Date'] = pd.to_datetime(dtf['Date'])\n",
        "dtf['month'] = dtf['Date'].dt.month\n",
        "dtf['year'] = dtf['Date'].dt.year\n",
        "dtf['week'] = dtf['Date'].dt.isocalendar().week\n",
        "dtf = dtf.drop('Date', axis=1)"
      ],
      "metadata": {
        "id": "kcGieFYiuCmT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}